# ═══════════════════════════════════════════════════════════════════
# P302 SLFB 鋅空氣電池 ── 正式模擬訓練配置 v4
# 架構: 純 SAC（訓練時不用 SafetyNet，deployment 時再啟用）
# ─────────────────────────────────────────────────────────────────
# v4 修正：
#   - v2/v3 失敗原因：SafetyNet 在訓練時覆寫 575/576 步動作
#     因為 P302 電池極小（一步 ΔSoC 可達 60%），SafetyNet 的 SoC
#     投影幾乎把每個動作壓成 0，agent 完全無法學習
#   - 解法：訓練用 variant=sac（純 SAC），靠 reward 引導安全行為
#     deployment 時再套 SafetyNet 做硬護欄
#   - alpha clamp 收緊到 [-5, 0]（max alpha=1.0）
# ═══════════════════════════════════════════════════════════════════

random_seed: 42

device: "auto"   # auto / cpu / cuda

# ── 環境參數（與 P302 實體規格一致）────────────────────────────────
env:
  microgrid_id: 0
  # 576 步 = 6天 × 96步/天（15分鐘一步），與 training CSV 筆數一致
  episode_length: 576
  time_step: 0.25               # 15分鐘 = 0.25小時

  # P302 SLFB 電池規格
  # 10 mAh @ ~7V nominal ≈ 0.07 Wh = 0.00007 kWh
  battery_capacity_kwh: 0.00007
  # 最大功率 20mA × 8.5V = 170mW ≈ 0.00017 kW
  battery_power_kw: 0.00017
  battery_efficiency: 0.95       # 功率損耗 5%（不加噪音/lag 等 sim-to-real 設定）

  use_real_data: true
  # 使用處理好的 6 天 15 分鐘 CSV（相對於 core/ 目錄）
  dataset_csv_path: "../data/processed/training_7day_15min.csv"
  dataset_pv_join_wind: false

  # CSV 欄位映射
  dataset_time_column: "timestamp"

  # P302 電池太小（0.07Wh），電網套利無意義
  # 使用 no-grid reward → 聚焦 SoC 管理 + PV 利用
  allow_grid_trading: false

  # 無 ramp limit（電池功率已很小，且不用 sim-to-real 噪音）
  ramp_limit_kw: null
  hard_guard: false

  # ── Flow Rate 電化學等效模型（SLFB 鋅空氣電池）────────────────────
  # 啟用 2D action space: [power_kw, flow_fraction]
  use_flow_rate_action: true
  # 基線內阻 R_base = (V_charge - V_discharge) / (2 × I_rated)
  # = (8.5 - 5.6) / (2 × 0.02) = 72.5 Ω
  flow_R_base_ohm: 72.5
  # 幫浦最大寄生功率 ≈ 16.8 mW = 0.0168 W
  flow_P_max_pump_W: 0.0168
  # 內阻增幅因子（低流速 → 濃度極化 → 內阻飆升）
  flow_k_R: 0.5
  # 開路電壓
  flow_V_OCV_charge: 8.5
  flow_V_OCV_discharge: 5.6
  # 額定電流
  flow_I_rated_A: 0.020

  # ── 擴充觀察空間 ────────────────────────────────────────────
  # 暫時關閉（SoH 資料在正式實驗前不夠精確）
  use_extended_obs: false

# ── SAC 超參數 ──────────────────────────────────────────────────
sac:
  actor_lr: 0.0003
  critic_lr: 0.0003
  gamma: 0.99
  tau: 0.005
  alpha: 0.05                   # 初始熵溫度（低，鼓勵早期收斂）
  target_entropy: -2.0          # 標準值 = -action_dim（無 SafetyNet 壓縮）
  alpha_lr: 0.0001              # alpha 獨立 lr（比 actor 3e-4 低，緩慢調整）
  hidden_dim: 128               # P302 任務不需太大網路
  buffer_size: 100000
  batch_size: 256
  warmup_steps: 2000            # 用隨機策略先填滿 buffer
  update_every: 4               # 每 4 步更新一次

# ── 訓練設定 ────────────────────────────────────────────────────
training:
  total_episodes: 2000          # 2000 集（每集 576 步 = ~115萬步總量）
  max_steps: 576                # 與 episode_length 一致
  eval_every: 25                # 每 25 集評估一次（更頻繁看進度）
  eval_episodes: 3              # 評估時跑 3 集取平均
  save_every: 100               # 每 100 集存 checkpoint

  # ★ v4: 純 SAC（不用 SafetyNet）
  # 原因：P302 電池一步 ΔSoC=60%，SafetyNet 覆寫 99.8% 的動作，學不了
  # Deployment 時再套 SafetyNet 做安全護欄
  variant: "sac"                # sac / sac_sn / sac_sn_evi

  # Evidential 超參（variant=sac_sn_evi 時使用）
  lambda_evi: 0.001
  beta_risk: 0.5

# ── SafetyNet（僅 deployment 時使用）──────────────────────────
safetynet:
  ramp_limit_kw: null

# ── Conformal Prediction（純 SAC 模式下不影響訓練）──────────────
conformal:
  window: 2880                  # 殘差窗口大小（≈5集的步數）
  delta: 0.1                    # 初始覆蓋率 = 1 - delta = 90%

# ── Reward 調整 ─────────────────────────────────────────────
reward:
  # v4: 無 SafetyNet → 這些 penalty 不會生效
  # 保留設定值供日後需要
  realized_violation_penalty: 10.0
  attempted_violation_penalty: 0.0
  safety_projection_penalty: 0.0

# ── Stress Testing（不使用）──────────────────────────────────
stress:
  enable: false

# ── 日誌與輸出 ───────────────────────────────────────────────
logging:
  log_interval: 10              # 每 10 集列印進度摘要
  save_models: true
  save_metrics: true
  plot_results: true
  csv_per_episode: true         # 每集寫一行 CSV（即時追蹤）